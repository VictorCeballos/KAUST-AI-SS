{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHCXeQOamHU1"
   },
   "source": [
    "#### Batching Sentences\n",
    "\n",
    "We have learned about batches in lecture. Waiting our whole training corpus to be processed before making an update is costly. On the other hand, updating the parameters after every training example causes the loss to be less stable between updates. To combat these issues, we instead update our parameters after training on a batch of data. This allows us to get a better estimate of the gradient of the global loss. In this section, we will learn how to structure our data into batches using the `torch.util.data.DataLoader` class.\n",
    "\n",
    "We will be calling the `DataLoader` class as follows: `DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`.  The `batch_size` parameter determines the number of examples per batch. In every epoch, we will be iterating over all the batches using the `DataLoader`. The order of batches is deterministic by default, but we can ask `DataLoader` to shuffle the batches by setting the `shuffle` parameter to `True`. This way we ensure that we don't encounter a bad batch multiple times.\n",
    "\n",
    "If provided, `DataLoader` passes the batches it prepares to the `collate_fn`. We can write a custom function to pass to the `collate_fn` parameter in order to print stats about our batch or perform extra processing. In our case, we will use the `collate_fn` to:\n",
    "1. Window pad our train sentences.\n",
    "2. Convert the words in the training examples to indices.\n",
    "3. Pad the training examples so that all the sentences and labels have the same length. Similarly, we also need to pad the labels. This creates an issue because when calculating the loss, we need to know the actual number of words in a given example. We will also keep track of this number in the function we pass to the `collate_fn` parameter.\n",
    "\n",
    "Because our version of the `collate_fn` function will need to access to our `word_to_ix` dictionary (so that it can turn words into indices), we will make use of the `partial` function in `Python`, which passes the parameters we give to the function we pass it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OkvvVlo4jgFm"
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from functools import partial\n",
    "\n",
    "# def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "#   # Break our batch into the training examples (x) and labels (y)\n",
    "#   # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence\n",
    "#   # method expects tensors. This is also useful since our model will be\n",
    "#   # expecting tensor inputs.\n",
    "#   x, y = zip(*batch)\n",
    "\n",
    "#   # Now we need to window pad our training examples. We have already defined a\n",
    "#   # function to handle window padding. We are including it here again so that\n",
    "#   # everything is in one place.\n",
    "#   def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "#     window = [pad_token] * window_size\n",
    "#     return window + sentence + window\n",
    "\n",
    "#   # Pad the train examples.\n",
    "#   x = [pad_window(s, window_size=window_size) for s in x]\n",
    "\n",
    "#   # Now we need to turn words in our training examples to indices. We are\n",
    "#   # copying the function defined earlier for the same reason as above.\n",
    "#   def convert_tokens_to_indices(sentence, word_to_ix):\n",
    "#     return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "#   # Convert the train examples into indices.\n",
    "#   x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "#   # We will now pad the examples so that the lengths of all the example in\n",
    "#   # one batch are the same, making it possible to do matrix operations.\n",
    "#   # We set the batch_first parameter to True so that the returned matrix has\n",
    "#   # the batch as the first dimension.\n",
    "#   pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "\n",
    "#   # pad_sequence function expects the input to be a tensor, so we turn x into one\n",
    "#   x = [torch.LongTensor(x_i) for x_i in x]\n",
    "#   x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "#   # We will also pad the labels. Before doing so, we will record the number\n",
    "#   # of labels so that we know how many words existed in each example.\n",
    "#   lengths = [len(label) for label in y]\n",
    "#   lenghts = torch.LongTensor(lengths)\n",
    "\n",
    "#   y = [torch.LongTensor(y_i) for y_i in y]\n",
    "#   y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "#   # We are now ready to return our variables. The order we return our variables\n",
    "#   # here will match the order we read them in our training loop.\n",
    "#   return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q0gXea-bCsz"
   },
   "source": [
    "This function seems long, but it really doesn't have to be. Check out the alternative version below where we remove the extra function declarations and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dZfcmAJXbLcq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def convert_tokens_to_indices(sentence, word_to_ix):\n",
    "  return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
    "  # Prepare the datapoints\n",
    "  x, y = zip(*batch)\n",
    "  x = [pad_window(s, window_size=window_size) for s in x]\n",
    "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "  # Pad x so that all the examples in the batch have the same size\n",
    "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "  x = [torch.LongTensor(x_i) for x_i in x]\n",
    "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "  # Pad y and record the length\n",
    "  lengths = [len(label) for label in y]\n",
    "  lenghts = torch.LongTensor(lengths)\n",
    "  y = [torch.LongTensor(y_i) for y_i in y]\n",
    "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "  return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS1WuQO0Khxx"
   },
   "source": [
    "Now, we can see the `DataLoader` in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfB0JKL2vZ6p",
    "outputId": "663b5215-01b3-475f-edb6-ad29186a967c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0],\n",
      "        [ 0,  0,  9,  7,  8, 18,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4, 4])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0],\n",
      "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([5, 5])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters to be passed to the DataLoader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn) ### Construct the dataloader with the decided BATCHSIZE\n",
    "\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "  print(f\"Iteration {counter}\")\n",
    "  print(\"Batched Input:\")\n",
    "  print(batched_x)\n",
    "  print(\"Batched Labels:\")\n",
    "  print(batched_y)\n",
    "  print(\"Batched Lengths:\")\n",
    "  print(batched_lengths)\n",
    "  print(\"\")\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93QOZSsMTFNF"
   },
   "source": [
    "The batched input tensors you see above will be passed into our model. On the other hand, we started off saying that our model will be a window classifier. The way our input tensors are currently formatted, we have all the words in a sentence in one datapoint. When we pass this input to our model, it needs to create the windows for each word, make a prediction as to whether the center word is a `LOCATION` or not for each window, put the predictions together and return.\n",
    "\n",
    "We could avoid this problem if we formatted our data by breaking it into windows beforehand. In this example, we will instead how our model take care of the formatting.\n",
    "\n",
    "Given that our `window_size` is `N` we want our model to make a prediction on every `2N+1` tokens. That is, if we have an input with `9` tokens, and a `window_size` of `2`, we want our model to return `5` predictions. This makes sense because before we padded it with `2` tokens on each side, our input also had `5` tokens in it!\n",
    "\n",
    "We can create these windows by using for loops, but there is a faster `PyTorch` alternative, which is the `unfold(dimension, size, step)` method. We can create the windows we need using this method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMZu-pxLVxHQ",
    "outputId": "2bff1c6a-62f5-46ab-e9d4-282ba405db6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: \n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 19,  5, 14],\n",
      "         [ 0, 19,  5, 14, 21],\n",
      "         [19,  5, 14, 21, 12],\n",
      "         [ 5, 14, 21, 12,  3],\n",
      "         [14, 21, 12,  3,  0],\n",
      "         [21, 12,  3,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "# Print the original tensor\n",
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "# Create the 2 * 2 + 1 chunks\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlDbOpeoSKxd"
   },
   "source": [
    "### Task-2: Model\n",
    "\n",
    "Now that we have prepared our data, we are ready to build our model using a `nn.Module` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JLTU4h76NLYm"
   },
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module): ### The architecture consists of embeding followed by FC layers followed by a sigmoid\n",
    "                                      ### out =1 ==> location, 0 otherwise\n",
    "\n",
    "  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "    super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "    \"\"\" Instance variables \"\"\"\n",
    "    self.window_size = hyperparameters[\"window_size\"]\n",
    "    self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "    \"\"\" Embedding Layer\n",
    "    Takes in a tensor containing embedding indices, and returns the\n",
    "    corresponding embeddings. The output is of dim\n",
    "    (number_of_indices * embedding_dim).\n",
    "\n",
    "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
    "    non-trainable. This is useful if we only want the parameters other than the\n",
    "    embeddings parameters to change.\n",
    "\n",
    "    \"\"\"\n",
    "    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "    if self.freeze_embeddings:\n",
    "      self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "    \"\"\" Hidden Layer\n",
    "    \"\"\"\n",
    "    full_window_size = 2 * window_size + 1\n",
    "    self.hidden_layer = nn.Sequential(\n",
    "      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "\n",
    "    \"\"\" Output Layer\n",
    "    \"\"\"\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    \"\"\" Probabilities\n",
    "    \"\"\"\n",
    "    self.probabilities = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    \"\"\"\n",
    "    Let B:= batch_size\n",
    "        L:= window-padded sentence length\n",
    "        D:= self.embed_dim\n",
    "        S:= self.window_size\n",
    "        H:= self.hidden_dim\n",
    "\n",
    "    inputs: a (B, L) tensor of token indices\n",
    "    \"\"\"\n",
    "    B, L = inputs.size()\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L) LongTensor\n",
    "    Outputs a (B, L~, S) LongTensor\n",
    "    \"\"\"\n",
    "    # First, get our word windows for each word in our input.\n",
    "    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "    # token_windows = None #To Do: Task-2a \n",
    "    _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1), 'failed simple test'\n",
    "\n",
    "    \"\"\"\n",
    "    Embedding.\n",
    "    Takes in a torch.LongTensor of size (B, L~, S)\n",
    "    Outputs a (B, L~, S, D) FloatTensor.\n",
    "    \"\"\"\n",
    "    embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L~, S, D) FloatTensor.\n",
    "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "    \"\"\"\n",
    "    B_t, L_t, S_t, D_t = embedded_windows.size()\n",
    "    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "    # embedded_windows = None #To Do: Task-2b\n",
    "    assert embedded_windows.size() == (B_t, L_t, S_t*D_t), 'failed simple test'\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 1.\n",
    "    Takes in a (B, L~, S*D) FloatTensor.\n",
    "    Resizes it into a (B, L~, H) FloatTensor\n",
    "    \"\"\"\n",
    "    layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 2\n",
    "    Takes in a (B, L~, H) FloatTensor.\n",
    "    Resizes it into a (B, L~, 1) FloatTensor.\n",
    "    \"\"\"\n",
    "    output = self.output_layer(layer_1)\n",
    "    # output = None #To Do: Task-2c\n",
    "\n",
    "    \"\"\"\n",
    "    Softmax.\n",
    "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
    "    \"\"\"\n",
    "    output = self.probabilities(output)\n",
    "    # output = None #To Do: Task-2d\n",
    "    output = output.view(B, -1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avy1fnyAvEcd"
   },
   "source": [
    "### Training\n",
    "\n",
    "We are now ready to put everything together. Let's start with preparing our data and intializing our model. We can then intialize our optimizer and define our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bInu1VqjHsfj"
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size) ### before trainig start, construct the model\n",
    "\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  ### set the optimizer that will help in updating the model parameters\n",
    "\n",
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()       \n",
    "    loss = bceloss(batch_outputs, batch_labels.float())  ### calculate the binary loss entropy\n",
    "    # loss = None #To do: Task-2e\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the\n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHxpxDkFHfQE"
   },
   "source": [
    "We will be using batches when passing our training data to the model in each epoch. Hence, in each training epoch iteration, we also iterate over the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QL9IDgIOvHca"
   },
   "outputs": [],
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "\n",
    "  # Keep track of the total loss for the batch\n",
    "  total_loss = 0\n",
    "  for batch_inputs, batch_labels, batch_lengths in loader: ### for each batch do:\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    outputs = model.forward(batch_inputs)   ### forward pass\n",
    "    # Compute the batch loss\n",
    "    loss = loss_function(outputs, batch_labels, batch_lengths) ### calculate the loss\n",
    "    # loss = None #To Do: Task-2f\n",
    "    # Calculate the gradients\n",
    "    loss.backward()   ### backpropogate\n",
    "    # Update the parameteres\n",
    "    optimizer.step()   ### Update the model's parameters\n",
    "    total_loss += loss.item()   ### get the loss value\n",
    "\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "# Function containing our main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):   ### start the training\n",
    "\n",
    "  # Iterate through each epoch and call our train_epoch function\n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "    if epoch % 100 == 0: print(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjf75cnzJ4n6"
   },
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kav8kwVBJ6XW",
    "outputId": "28dafd55-14b7-4a21-b7d0-fd8cde687fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31437375396490097\n",
      "0.21641439944505692\n",
      "0.16034535318613052\n",
      "0.14019093289971352\n",
      "0.09667090140283108\n",
      "0.08393667452037334\n",
      "0.062930166721344\n",
      "0.04881000146269798\n",
      "0.04304482601583004\n",
      "0.03137654112651944\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-k7Pav4LdQJ"
   },
   "source": [
    "### Task-3: Prediction\n",
    "\n",
    "Let's see how well our model is at making predictions. We can start by creating our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-v5X69a2Lkbm"
   },
   "outputs": [],
   "source": [
    "# Create test sentences\n",
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "# Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlBa8xaNMZgv"
   },
   "source": [
    "Let's loop over our test examples to see how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGYn8CAoMTjX",
    "outputId": "e63fce32-9d1d-4a80-a2f9-dd555928f852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.2248, 0.0198, 0.1840, 0.9846]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:   ### performe prediction by simply a forward path\n",
    "  outputs = model.forward(test_instance)\n",
    "  # outputs = None #To Do: Task-3\n",
    "  print(labels)\n",
    "  print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
