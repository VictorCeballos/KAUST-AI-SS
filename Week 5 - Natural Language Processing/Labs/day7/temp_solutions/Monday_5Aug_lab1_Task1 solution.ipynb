{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8rUNk_1xG1v"
   },
   "source": [
    "## Word Window Classification\n",
    "\n",
    "In this lab we will attempt to solve a simple toy NLP task. Here are the things we will learn:\n",
    "\n",
    "1. Data: Creating a Dataset of Batched Tensors\n",
    "2. Modeling\n",
    "3. Training\n",
    "4. Prediction \n",
    "\n",
    "In this section, our goal will be to train a model that will find the words in a sentence corresponding to a `LOCATION`, which will be always of span `1` (meaning that `San Fransisco` won't be recognized as a `LOCATION`). Our task is called `Word Window Classification` for a reason. Instead of letting our model to only take a look at one word in each forward pass, we would like it to be able to consider the context of the word in question. That is, for each word, we want our model to be aware of the surrounding words. Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_amzuUx8BJXI"
   },
   "source": [
    "#### Data\n",
    "\n",
    "The very first task of any machine learning project is to set up our training set. Usually, there will be a training corpus we will be utilizing. In NLP tasks, the corpus would generally be a `.txt` or `.csv` file where each row corresponds to a sentence or a tabular datapoint. In our toy task, we will assume that we have already read our data and the corresponding labels into a `Python` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mDiI1PLMw10z"
   },
   "outputs": [],
   "source": [
    "### This is our training dataset :)\n",
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t33Uke9AE22s"
   },
   "source": [
    "#### Task-1: Preprocessing\n",
    "\n",
    "To make it easier for our models to learn, we usually apply a few preprocessing steps to our data. This is especially important when dealing with text data. Here are some examples of text preprocessing:\n",
    "* **Tokenization**: Tokenizing the sentences into words.\n",
    "* **Lowercasing**: Changing all the letters to be lowercase.\n",
    "* **Noise removal:** Removing special characters (such as punctuations).\n",
    "* **Stop words removal**: Removing commonly used words.\n",
    "\n",
    "Which preprocessing steps are necessary is determined by the task at hand. For example, although it is useful to remove special characters in some tasks, for others they may be important (for example, if we are dealing with multiple languages). For our task, we will lowercase our words and tokenize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTGn8ANTzZXT",
    "outputId": "0ed249ae-7272-4400-9319-d7cc9dca02d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The preprocessing function we will use to generate our training examples\n",
    "# Our function is a simple one, we lowercase the letters\n",
    "# and then tokenize the words.\n",
    "def preprocess_sentence(sentence):\n",
    "  return sentence.lower().split()\n",
    "  # return None #To Do: task-1a lowercase and tokenize\n",
    "\n",
    "# Create our training set\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "train_sentences ### lowercase and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple check task-1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "test_train_sentences = [['we', 'always', 'come', 'to', 'paris'],\n",
    "                    ['the', 'professor', 'is', 'from', 'australia'],\n",
    "                    ['i', 'live', 'in', 'stanford'],\n",
    "                    ['he', 'comes', 'from', 'taiwan'],\n",
    "                    ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]\n",
    "assert test_train_sentences == train_sentences, 'test failed!'\n",
    "print('Pass!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4jzo5tp0Hza"
   },
   "source": [
    "For each training example we have, we should also have a corresponding label. Recall that the goal of our model was to determine which words correspond to a `LOCATION`. That is, we want our model to output `0` for all the words that are not `LOCATION`s and `1` for the ones that are `LOCATION`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wo1kcMAHFw7",
    "outputId": "f064cb0b-7176-4825-bab9-f7c00accff3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of locations that appear in our corpus\n",
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "\n",
    "# Our train labels\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "# train_labels = None #To Do: task-1b produce the correct training labels\n",
    "train_labels  ### Construct the true labels of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple check task-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "test_train_labels = [[0, 0, 0, 0, 1],\n",
    "                        [0, 0, 0, 0, 1],\n",
    "                        [0, 0, 0, 1],\n",
    "                        [0, 0, 0, 1],\n",
    "                        [0, 0, 0, 1, 0, 1]]\n",
    "assert test_train_labels == train_labels, 'test failed!'\n",
    "print('Pass!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgVKH9M3RtPx"
   },
   "source": [
    "#### Converting Words to Embeddings\n",
    "\n",
    "Let's look at our training data a little more closely. Each datapoint we have is a sequence of words. On the other hand, we know that machine learning models work with numbers in vectors. How are we going to turn words into numbers? You may be thinking embeddings and you are right!\n",
    "\n",
    "Imagine that we have an embedding lookup table `E`, where each row corresponds to an embedding. That is, each word in our vocabulary would have a corresponding embedding row `i` in this table. Whenever we want to find an embedding for a word, we will follow these steps:\n",
    "1. Find the corresponding index `i` of the word in the embedding table: `word->index`.\n",
    "2. Index into the embedding table and get the embedding: `index->embedding`.\n",
    "\n",
    "Let's look at the first step. We should assign all the words in our vocabulary to a corresponding index. We can do it as follows:\n",
    "1. Find all the unique words in our corpus.\n",
    "2. Assign an index to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjTDlfPyVp5z",
    "outputId": "bce95386-3b75-470b-8c9b-9c2255203c4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the unique words in our corpus\n",
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "# vocabulary = None #To Do: task-1c find the unique words from the training set\n",
    "vocabulary   ### construct our unique vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple check task-1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "test_vocabulary = {'always',\n",
    "                    'ankara',\n",
    "                    'australia',\n",
    "                    'capital',\n",
    "                    'come',\n",
    "                    'comes',\n",
    "                    'from',\n",
    "                    'he',\n",
    "                    'i',\n",
    "                    'in',\n",
    "                    'is',\n",
    "                    'live',\n",
    "                    'of',\n",
    "                    'paris',\n",
    "                    'professor',\n",
    "                    'stanford',\n",
    "                    'taiwan',\n",
    "                    'the',\n",
    "                    'to',\n",
    "                    'turkey',\n",
    "                    'we'}\n",
    "assert test_vocabulary == vocabulary, 'test failed!'\n",
    "print('Pass!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOxnKznOWXSC"
   },
   "source": [
    "`vocabulary` now contains all the words in our corpus. On the other hand, during the test time, we can see words that are not contained in our vocabulary. If we can figure out a way to represent the unknown words, our model can still reason about whether they are a `LOCATION` or not, since we are also looking at the neighboring words for each prediction.\n",
    "\n",
    "We introduce a special token, `<unk>`, to tackle the words that are out of vocabulary. We could pick another string for our unknown token if we wanted. The only requirement here is that our token should be unique: we should only be using this token for unknown words. We will also add this special token to our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ygxYuE1DYeR3"
   },
   "outputs": [],
   "source": [
    "# Add the unknown token to our vocabulary\n",
    "vocabulary.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsf4haL94AFu"
   },
   "source": [
    "Earlier we mentioned that our task was called `Word Window Classification` because our model is looking at the surroundings words in addition to the given word when it needs to make a prediction.\n",
    "\n",
    "For example, let's take the sentence \"We always come to Paris\". The corresponding training label for this sentence is `0, 0, 0, 0, 1` since only Paris, the last word, is a `LOCATION`. In one pass (meaning a call to `forward()`), our model will try to generate the correct label for one word. Let's say our model is trying to generate the correct label `1` for `Paris`. If we only allow our model to see `Paris`, but nothing else, we will miss out on the important information that the word `to` often times appears with `LOCATION`s.\n",
    "\n",
    "Word windows allow our model to consider the surrounding `+N` or `-N` words of each word when making a prediction. In our earlier example for `Paris`, if we have a window size of 1, that means our model will look at the words that come immediately before and after `Paris`, which are `to`, and, well, nothing. Now, this raises another issue. `Paris` is at the end of our sentence, so there isn't another word following it. Remember that we define the input dimensions of our `PyTorch` models when we are initializing them. If we set the window size to be `1`, it means that our model will be accepting `3` words in every pass. We cannot have our model expect `2` words from time to time.\n",
    "\n",
    "The solution is to introduce a special token, such as `<pad>`, that will be added to our sentences to make sure that every word has a valid window around them. Similar to `<unk>` token, we could pick another string for our pad token if we wanted, as long as we make sure it is used for a unique purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVQsjYi6ZegI",
    "outputId": "f0f45f18-0e91-48e5-f5b7-229c2c36a694"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the <pad> token to our vocabulary\n",
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "# Function that pads the given sentence\n",
    "# We are introducing this function here as an example\n",
    "# We will be utilizing it later in the tutorial\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "  window = [pad_token] * window_size\n",
    "  # window = None #To Do: task-1d add pad_token to the sentence considering the window size\n",
    "  return window + sentence + window\n",
    "\n",
    "# Show padding example\n",
    "window_size = 2\n",
    "test_pading = pad_window(train_sentences[0], window_size=window_size)\n",
    "test_pading   ### Do not forget to PAD your sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple check task-1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "crct_pad_vec = ['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']\n",
    "assert test_pading == crct_pad_vec, 'test failed!'\n",
    "print('Pass!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqvgWKwSNpAd"
   },
   "source": [
    "Now that our vocabularly is ready, let's assign an index to each of our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNCTQnKDa4oh",
    "outputId": "dedbe39a-40c2-42e8-be56-96b6d961fd99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are just converting our vocabularly to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_to_word = sorted(list(vocabulary))\n",
    "\n",
    "# Creating a dictionary to find the index of a given word\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "# word_to_ix = None #To Do: task-1e construct the nedded dic\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple check task-1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "test_word_to_ix = {'<pad>': 0,\n",
    "                    '<unk>': 1,\n",
    "                    'always': 2,\n",
    "                    'ankara': 3,\n",
    "                    'australia': 4,\n",
    "                    'capital': 5,\n",
    "                    'come': 6,\n",
    "                    'comes': 7,\n",
    "                    'from': 8,\n",
    "                    'he': 9,\n",
    "                    'i': 10,\n",
    "                    'in': 11,\n",
    "                    'is': 12,\n",
    "                    'live': 13,\n",
    "                    'of': 14,\n",
    "                    'paris': 15,\n",
    "                    'professor': 16,\n",
    "                    'stanford': 17,\n",
    "                    'taiwan': 18,\n",
    "                    'the': 19,\n",
    "                    'to': 20,\n",
    "                    'turkey': 21,\n",
    "                    'we': 22}\n",
    "assert test_word_to_ix == word_to_ix, 'test failed!'\n",
    "print('Pass!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELZuteqbdWd1"
   },
   "source": [
    "Great! We are ready to convert our training sentences into a sequence of indices corresponding to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNOxip15bMfH",
    "outputId": "74f8344f-bdeb-4a37-9442-b317013cdc10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "def convert_token_to_indices(sentence, word_to_ix): ### convert words into indices so emmbedings can be found later using nn.Embedding\n",
    "  indices = []\n",
    "  for token in sentence:\n",
    "    # Check if the token is in our vocabularly. If it is, get it's index.\n",
    "    # If not, get the index for the unknown token.\n",
    "    # To Do: task-1f\n",
    "    # pass\n",
    "    if token in word_to_ix:\n",
    "      index = word_to_ix[token]\n",
    "    else:\n",
    "      index = word_to_ix[\"<unk>\"]\n",
    "    indices.append(index)\n",
    "  return indices\n",
    "\n",
    "# More compact version of the same function\n",
    "def _convert_token_to_indices(sentence, word_to_ix):\n",
    "  return [word_to_ind.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "# Show an example\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple check task-1f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass!\n"
     ]
    }
   ],
   "source": [
    "assert ['we', 'always', 'come', 'to', 'kuwait'] == example_sentence, 'test failed!'\n",
    "assert [22, 2, 6, 20, 1] == example_indices, 'test failed!'\n",
    "assert ['we', 'always', 'come', 'to', '<unk>'] == restored_example, 'test failed!'\n",
    "print('Pass!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jsXw8cB1xpH"
   },
   "source": [
    "In the example above, `kuwait` shows up as `<unk>`, because it is not included in our vocabulary. Let's convert our `train_sentences` to `example_padded_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRaKQwSJH-1d",
    "outputId": "ac2c370a-4d6e-41a7-afd2-652c76f6c235"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting our sentences to indices\n",
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZULjHBjHsEK"
   },
   "source": [
    "Now that we have an index for each word in our vocabularly, we can create an embedding table with `nn.Embedding` class in `PyTorch`. It is called as follows `nn.Embedding(num_words, embedding_dimension)` where `num_words` is the number of words in our vocabulary and the `embedding_dimension` is the dimension of the embeddings we want to have. There is nothing fancy about `nn.Embedding`: it is just a wrapper class around a trainabe `NxE` dimensional tensor, where `N` is the number of words in our vocabulary and `E` is the number of embedding dimensions. This table is initially random, but it will change over time. As we train our network, the gradients will be backpropagated all the way to the embedding layer, and hence our word embeddings would be updated. We will initiliaze the embedding layer we will use for our model in our model, but we are showing an example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4AgHzv91VXx",
    "outputId": "5404f7e3-919b-4e98-9411-0f92bd5248e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-6.4225e-01, -1.1382e+00,  2.4819e-01,  4.2047e-01,  5.3647e-01],\n",
       "         [ 7.3446e-01,  1.5653e+00, -4.2861e-01,  5.3950e-01, -2.4080e+00],\n",
       "         [-1.0467e+00,  4.5272e-04,  5.1691e-01,  9.7846e-01,  3.4240e-01],\n",
       "         [ 9.7249e-01, -1.8092e+00, -9.6482e-01,  8.0514e-01, -1.2050e+00],\n",
       "         [-4.2263e-01, -1.4729e+00, -6.5408e-01, -7.5241e-01,  1.5103e+00],\n",
       "         [ 4.2020e-01, -2.7646e-01,  4.1917e-01, -9.5254e-01, -9.9322e-01],\n",
       "         [ 1.2536e+00,  1.2376e+00,  8.3681e-01, -1.5655e+00, -3.7557e-01],\n",
       "         [-5.6801e-01, -1.0017e+00, -4.0199e-01,  1.5494e+00,  1.6790e-01],\n",
       "         [ 1.0326e+00,  4.4658e-01, -1.2431e+00, -1.1461e+00,  8.2310e-02],\n",
       "         [-2.5752e-01, -9.1020e-01,  5.7974e-01,  1.6198e+00, -1.1124e-03],\n",
       "         [ 3.5792e-01,  1.4910e-02, -1.0764e+00, -2.2492e-01, -3.8519e-01],\n",
       "         [ 7.4097e-01,  1.4309e+00, -1.5491e+00, -5.7491e-01,  8.5422e-01],\n",
       "         [ 4.4444e-01,  1.7229e+00,  4.9764e-01,  6.2893e-01, -6.4231e-01],\n",
       "         [-9.2232e-01,  8.2081e-02,  7.1906e-01,  9.0736e-01, -5.8964e-01],\n",
       "         [-4.9249e-01, -1.5993e+00,  1.1791e+00,  5.4320e-01,  5.5043e-01],\n",
       "         [ 2.7388e+00,  5.1303e-01,  7.2207e-02, -1.0114e-01, -8.4716e-01],\n",
       "         [-1.9589e+00, -8.2606e-03,  1.0693e+00,  1.3048e+00, -7.7420e-01],\n",
       "         [ 4.9596e-01,  1.1751e+00, -5.2678e-01,  5.9496e-01, -1.3429e-01],\n",
       "         [-2.4172e+00,  1.7186e+00, -1.0171e+00, -2.6198e+00, -2.5057e+00],\n",
       "         [-4.4722e-01, -1.5526e-01, -3.5490e-01,  1.0391e+00,  8.9764e-01],\n",
       "         [-5.7497e-01, -8.2462e-01,  5.7477e-01, -8.3154e-01,  2.8682e-01],\n",
       "         [ 1.2888e-01,  2.7800e-01,  8.7662e-02, -4.6132e-01,  1.4789e+00],\n",
       "         [ 1.5539e+00, -4.9993e-01,  1.0229e+00, -7.6914e-01, -3.2520e-01]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an embedding table for our words\n",
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "# embeds = None #To Do: task-1g use nn.Embedding\n",
    "\n",
    "# Printing the parameters in our embedding table\n",
    "assert len(list(embeds.parameters())[0]) == 23, 'simple test failed!'\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI7ZTt4OkpPp"
   },
   "source": [
    "To get the word embedding for a word in our vocabulary, all we need to do is to create a lookup tensor. The lookup tensor is just a tensor containing the index we want to look up `nn.Embedding` class expects an index tensor that is of type Long Tensor, so we should create our tensor accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkldmcepjfh_",
    "outputId": "b9e24c31-547d-41e5-9509-fb9e9777cd01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7388,  0.5130,  0.0722, -0.1011, -0.8472],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUsdwBOxm6B4",
    "outputId": "5b8cb8fe-0787-4e93-946b-ed719efcdd8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7388,  0.5130,  0.0722, -0.1011, -0.8472],\n",
       "        [ 0.9725, -1.8092, -0.9648,  0.8051, -1.2050]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get multiple embeddings at once\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bgSW3LPltkF"
   },
   "source": [
    "Usually, we define the embedding layer as part of our model, which you will see in the later sections of the notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
