{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "2OoUWgrFi82h",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:17.628176Z",
     "start_time": "2024-07-04T07:08:17.621705Z"
    }
   },
   "source": [
    "from IPython.display import clear_output, display"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MN1Z7uGbxTdq",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:18.137598Z",
     "start_time": "2024-07-04T07:08:18.134030Z"
    }
   },
   "source": [
    "# %pip install torch torchvision pillow spacy numpy\n",
    "# %pip install torchtext\n",
    "# %pip install pycocotools"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5EDyhvMIxybA",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:22.346376Z",
     "start_time": "2024-07-04T07:08:18.496131Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import spacy"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/pythonProject/.venv/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_4aD7hcPxTdr",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:22.353409Z",
     "start_time": "2024-07-04T07:08:22.348667Z"
    }
   },
   "source": [
    "dataset_variant = 'val2017'"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wijrNMlqxi5R"
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rlPorEo6jseG"
   },
   "source": [
    "# Define paths for dataset and annotations\n",
    "data_dir = './data'\n",
    "images_dir = os.path.join(data_dir, dataset_variant)\n",
    "annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)\n",
    "if not os.path.exists(annotations_dir):\n",
    "    os.makedirs(annotations_dir)\n",
    "\n",
    "# Download dataset\n",
    "#!wget http://images.cocodataset.org/zips/{dataset_variant}.zip -P {data_dir}\n",
    "\n",
    "# Unzip dataset\n",
    "# !unzip {data_dir}/{dataset_variant}.zip -d {data_dir}\n",
    "\n",
    "clear_output()\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:31.791401Z",
     "start_time": "2024-07-04T07:08:31.788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download annotations\n",
    "# !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P {annotations_dir}"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11716,
     "status": "ok",
     "timestamp": 1711150521429,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "UQeFlwUfC-U0",
    "outputId": "8f291b9a-80a6-4e4a-8257-66ac01b6a0db",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:34.285422Z",
     "start_time": "2024-07-04T07:08:34.281481Z"
    }
   },
   "source": [
    "# # Unzip annotations\n",
    "# !unzip {annotations_dir}/annotations_trainval2017.zip -d {annotations_dir}"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYIiLwcix_8n"
   },
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711150521429,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "JwP-q5wjmo4b",
    "outputId": "3076551a-bc5d-4095-8e27-fa5ec3d741c7",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:36.027808Z",
     "start_time": "2024-07-04T07:08:35.944165Z"
    }
   },
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Load MS-COCO dataset\n",
    "train_dataset = CocoCaptions(root=f'./data/{dataset_variant}', annFile=f'./data/annotations/annotations/captions_{dataset_variant}.json', transform=transform)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzeLAtu9zQFv"
   },
   "source": [
    "## Building the tokenizer and vocabulary"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# !python -m spacy download en_core_web_sm",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I2kg8R_nhN1A",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:41.486161Z",
     "start_time": "2024-07-04T07:08:40.671662Z"
    }
   },
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y-3CxVtzxcd6",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:08:42.342702Z",
     "start_time": "2024-07-04T07:08:42.339241Z"
    }
   },
   "source": [
    "def word_tokenize(text):\n",
    "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40766,
     "status": "ok",
     "timestamp": 1711150562735,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "_JQ8Ow_t7B7P",
    "outputId": "8375106b-8e90-4ef7-d12a-d5090bb597d1",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:10.499701Z",
     "start_time": "2024-07-04T07:08:42.953972Z"
    }
   },
   "source": [
    "# Define the vocabulary and tokenizer\n",
    "word_to_index = {'<PAD>':0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "index_to_word = {it: k for k, it in word_to_index.items()}\n",
    "word_freq = {}\n",
    "caption_lengths = []\n",
    "\n",
    "\n",
    "# Tokenize captions and build vocabulary\n",
    "for _, captions in tqdm(train_dataset):\n",
    "    for caption in captions:\n",
    "        caption = f'{caption}'\n",
    "        caption_lengths.append(len(caption))\n",
    "        tokens = word_tokenize(caption.lower())\n",
    "        for token in tokens:\n",
    "            if token not in word_to_index:\n",
    "                idx = len(word_to_index)\n",
    "                word_to_index[token] = idx\n",
    "                index_to_word[idx] = token\n",
    "                word_freq[token] = 1\n",
    "            else:\n",
    "                word_freq[token] += 1"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:27<00:00, 181.59it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1711150563501,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "Mbv23I3-xUee",
    "outputId": "0700ac2b-b969-4405-f637-ba9a10b77478",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:10.508684Z",
     "start_time": "2024-07-04T07:09:10.501454Z"
    }
   },
   "source": [
    "word_tokenize('<SOS> hi, my friend <EOS>')  # We will manually add tokens for <EOS> and <SOS> etc after tokenization to avoid them breaking up."
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'sos', '>', 'hi', ',', 'my', 'friend', '<', 'eos', '>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i79_gIk-1zut"
   },
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D59I4JSZe46D",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:10.520473Z",
     "start_time": "2024-07-04T07:09:10.510400Z"
    }
   },
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        self.resnet = models.resnet50(pretrained=True).requires_grad_(False)  # resnet embedding backbone\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        features = self.resnet(images)\n",
    "        # features = features[0] if not isinstance(features, torch.Tensor) else features\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.Linear(1024, vocab_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=-2)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(0)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if index_to_word[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [index_to_word[idx] for idx in result_caption]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1711150563502,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "Dsnhr5G5L01M",
    "outputId": "d8e897ff-bebf-4a48-e3fe-4276b672b05a",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:10.528282Z",
     "start_time": "2024-07-04T07:09:10.522452Z"
    }
   },
   "source": [
    "def convert_sentence_to_idxs(sentence):\n",
    "\n",
    "    words = word_tokenize(sentence)\n",
    "    idxs = [word_to_index[word] for word in words]\n",
    "\n",
    "    return idxs\n",
    "\n",
    "\n",
    "def convert_idxs_to_sentence(idxs):\n",
    "\n",
    "    words = [index_to_word[idx] for idx in idxs]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# Need to define a collate function which pads the sentence tokens in the batch to be of the same length so they can be stacked.\n",
    "# We also take care of converting string tokens to idxs here.\n",
    "def collate_fn(data):\n",
    "\n",
    "    images, captions = zip(*data)\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    captions = [f'{caption[0]}' for caption in captions]  # each image has multiple captions. we use just the first one here.\n",
    "\n",
    "    # manually adding <SOS> and <EOS> tokens after tokenization and conversion because our tokenizers break <SOS> and <EOS>\n",
    "    captions = [torch.Tensor([word_to_index['<SOS>']]+convert_sentence_to_idxs(caption.lower().strip())+[word_to_index['<EOS>']]) for caption in captions]\n",
    "\n",
    "    # 0 is the idx for <PAD> (see index_to_word)\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    return images, captions\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1711150563502,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "rlsyakwKrQ2Z",
    "outputId": "764f0721-6f6f-4ee4-968f-75cc2a8ae509",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:10.535084Z",
     "start_time": "2024-07-04T07:09:10.530094Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5YB-RYSH2DmL",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:10.540040Z",
     "start_time": "2024-07-04T07:09:10.537080Z"
    }
   },
   "source": [
    "embed_size = 2048\n",
    "hidden_size = 256\n",
    "vocab_size = len(word_to_index)\n",
    "num_lstm_layers = 1\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 100"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1411,
     "status": "ok",
     "timestamp": 1711150564908,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "v7-7f4gmhr5u",
    "outputId": "b8178133-a616-41ee-fe98-0e82251b5619",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:11.803017Z",
     "start_time": "2024-07-04T07:09:10.541581Z"
    }
   },
   "source": [
    "# initialize model, loss etc\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_lstm_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index['<PAD>'])  # ignore pad token loss calculations\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Only finetune the CNN\n",
    "for name, param in model.encoderCNN.resnet.named_parameters():\n",
    "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/pythonProject/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ali/pythonProject/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T07:09:12.772743Z",
     "start_time": "2024-07-04T07:09:11.804434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading the model (example)\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_lstm_layers).to(device)  # Initialize the model\n",
    "model.load_state_dict(torch.load('lstm-model.pth'))  # Load the saved state dictionary\n",
    "model.to(device)  # Move the model to the appropriate device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNtoRNN(\n",
       "  (encoderCNN): EncoderCNN(\n",
       "    (resnet): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoderRNN): DecoderRNN(\n",
       "    (embed): Embedding(7231, 2048)\n",
       "    (lstm): LSTM(2048, 256, batch_first=True)\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): Linear(in_features=1024, out_features=7231, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db6Iq3hU3GMk"
   },
   "source": [
    "## Pre-training Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QtS8hnpH3K3D",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:12:50.688101Z",
     "start_time": "2024-07-04T07:12:50.666853Z"
    }
   },
   "source": [
    "test_img_paths = ['data/val2017/000000000139.jpg', 'data/val2017/000000000632.jpg', 'data/val2017/000000000724.jpg']\n",
    "imgs_pil = [Image.open(path).convert('RGB') for path in test_img_paths]\n",
    "imgs_test = [transform(im_pil).to(device) for im_pil in imgs_pil]"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1711150566821,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "xiAYu3QXtHki",
    "outputId": "838ee1ad-1aae-400c-806a-88b1e2d88a64",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:12:51.819457Z",
     "start_time": "2024-07-04T07:12:50.988809Z"
    }
   },
   "source": [
    "\n",
    "imgs = torch.stack(imgs_test, 0)\n",
    "model.eval()\n",
    "captions = []\n",
    "\n",
    "for img in imgs_test:\n",
    "    with torch.no_grad():\n",
    "        caption = model.caption_image(img.unsqueeze(0))\n",
    "        caption = ' '.join(caption)\n",
    "        captions.append(caption)\n",
    "\n",
    "print(('\\n'+'-'*20+'\\n').join(captions))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/pythonProject/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> a living room filled with furniture and a flat screen tv . <EOS>\n",
      "--------------------\n",
      "<SOS> a living room filled with furniture and a flat screen tv . <EOS>\n",
      "--------------------\n",
      "<SOS> a red stop sign sitting on the side of a road . <EOS>\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1711150568365,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "LbMc_3RrxTdv",
    "outputId": "5b6a9865-66dc-4c32-bb27-0c7eda570330",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:13:10.586523Z",
     "start_time": "2024-07-04T07:13:10.560426Z"
    }
   },
   "source": "imgs_pil[2]",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mimgs_pil\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOQLLt_F3iIs"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4856041,
     "status": "ok",
     "timestamp": 1711155424403,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "TpeCGaogiE0Q",
    "outputId": "26242b59-a8f2-4ee9-9f2f-8e8310353649"
   },
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idx, (imgs, captions) in tqdm(\n",
    "        enumerate(train_loader), total=len(train_loader), leave=False\n",
    "    ):\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device).type(torch.long)\n",
    "\n",
    "        outputs = model(imgs, captions[:, :-1])\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), captions[:, :].reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # For debugging purposes\n",
    "    # model.eval()\n",
    "    # captions = []\n",
    "\n",
    "    # for img in imgs_test:\n",
    "    #     with torch.no_grad():\n",
    "    #         caption = model.caption_image(img.unsqueeze(0))\n",
    "    #         caption = ' '.join(caption)\n",
    "    #         captions.append(caption)\n",
    "    # print('\\n'.join(captions))\n",
    "\n",
    "    print(f'\\nEpoch: {epoch+1}/{num_epochs}', \"Training loss: \", loss.item())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1711155424403,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "yjthUG9nr9kE",
    "outputId": "b27fffb5-8ecf-4750-8d4b-12c1de279011"
   },
   "source": [
    "!ls data/val2017/ | head -20"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711145144726,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "75DqWjxWMmkO",
    "outputId": "44e017ea-144d-42aa-96d5-28f174f270d9"
   },
   "source": [
    "!ls data/"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1711155485861,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "wiEdECiYxTdv",
    "outputId": "ff7c8d2d-fec3-43be-b983-e0eef52d2981"
   },
   "source": [
    "img = Image.open('data/val2017/000000001490.jpg').convert('RGB')\n",
    "img"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1711155489421,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "a6qsa0YtTELG",
    "outputId": "000d8021-8561-450c-c717-9a48e1dd1e73"
   },
   "source": [
    "img_t = transform(img).to(device).unsqueeze(0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    caption = model.caption_image(img_t)\n",
    "\n",
    "caption"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1913,
     "status": "ok",
     "timestamp": 1711155919562,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "14169008838171441457"
     },
     "user_tz": -300
    },
    "id": "w5sSYjgbxTdw",
    "outputId": "022c8edd-acb4-4976-bc48-45bcf268fdad"
   },
   "source": [
    "for img, img_pil in zip(imgs_test, imgs_pil):\n",
    "    with torch.no_grad():\n",
    "        caption = model.caption_image(img.unsqueeze(0))\n",
    "        caption = ' '.join(caption)\n",
    "\n",
    "        display(img_pil)\n",
    "        print(caption)\n",
    "        print('-'*20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5w0eKDXR1Z6k"
   },
   "source": "torch.save(model.state_dict(),'lstm-model.pth')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
