{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "6XOp7t3n-KAm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hn9TBpwR-Egi"
      },
      "outputs": [],
      "source": [
        "class MHSA(nn.Module):\n",
        "    def __init__(self, n_dims, width=14, height=14, heads=4):\n",
        "        super(MHSA, self).__init__()\n",
        "        self.heads = heads\n",
        "        # Define the query, key, and value convolutions, each with kernel size of 1\n",
        "\n",
        "        self.query = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n",
        "        self.key = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n",
        "        self.value = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n",
        "        # Define the relative positional encodings for height and width\n",
        "        self.rel_h = nn.Parameter(torch.randn([1, heads, n_dims // heads, 1, height]), requires_grad=True)\n",
        "        self.rel_w = nn.Parameter(torch.randn([1, heads, n_dims // heads, width, 1]), requires_grad=True)\n",
        "        # Define the softmax layer for computing attention weights\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the input dimensions\n",
        "        n_batch, C, width, height = x.size()\n",
        "\n",
        "        # Compute query, key, and value matrices\n",
        "        q = self.query(x).view(n_batch, self.heads, C // self.heads, -1)\n",
        "        k = self.key(x).view(n_batch, self.heads, C // self.heads, -1)\n",
        "        v = self.value(x).view(n_batch, self.heads, C // self.heads, -1)\n",
        "\n",
        "        # Compute the content-content interaction\n",
        "        content_content = torch.matmul(q.permute(0, 1, 3, 2), k)\n",
        "\n",
        "        # Compute the content-position interaction using relative positional encodings\n",
        "        content_position = (self.rel_h + self.rel_w).view(1, self.heads, C // self.heads, -1).permute(0, 1, 3, 2)\n",
        "        content_position = torch.matmul(content_position, q)\n",
        "\n",
        "        # Combine content-content and content-position interactions\n",
        "        energy = content_content + content_position\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention = self.softmax(energy)\n",
        "\n",
        "\n",
        "        # Compute the output by applying attention weights to the value matrix\n",
        "        out = torch.matmul(v, attention.permute(0, 1, 3, 2))\n",
        "        out = out.view(n_batch, C, width, height)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the MHSA class\n",
        "n_dims = 4\n",
        "width = height = 14\n",
        "heads = 4\n",
        "mhsa = MHSA(n_dims, width, height, heads)\n",
        "\n",
        "# Create a random input tensor with shape (batch_size, channels, width, height)\n",
        "batch_size = 1\n",
        "input_tensor = torch.randn(batch_size, n_dims, width, height)\n",
        "\n",
        "# Pass the input tensor through the MHSA layer\n",
        "output_tensor = mhsa(input_tensor)\n",
        "\n",
        "# Print the shape of the input and output tensors\n",
        "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
        "print(\"Output Tensor Shape:\", output_tensor.shape)\n",
        "\n",
        "# Print the first 5 pixels of the input tensor\n",
        "print(\"First 5 pixels of the input tensor:\")\n",
        "print(input_tensor[0, :, 0, 0:5])  # Print the first 5 pixels of the first channel of the first batch element\n",
        "\n",
        "# Print the first 5 pixels of the output tensor\n",
        "print(\"First 5 pixels of the output tensor:\")\n",
        "print(output_tensor[0, :, 0, 0:5])  # Print the first 5 pixels of the first channel of the first batch element"
      ],
      "metadata": {
        "id": "voaJprOg-F8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509ad796-6013-4ebc-b32e-6afa8bc41fc1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor Shape: torch.Size([1, 4, 14, 14])\n",
            "Output Tensor Shape: torch.Size([1, 4, 14, 14])\n",
            "First 5 pixels of the input tensor:\n",
            "tensor([[ 2.5608, -1.8422,  0.9996, -1.5135,  0.3690],\n",
            "        [-2.1321,  0.7062,  0.0465,  0.8490,  1.2388],\n",
            "        [ 1.6879, -1.7443, -0.5342, -0.2844,  0.3720],\n",
            "        [ 0.8201, -0.1380,  1.3994,  1.2503, -0.1011]])\n",
            "First 5 pixels of the output tensor:\n",
            "tensor([[ 0.0041, -0.4108, -0.2220, -0.0871, -0.1768],\n",
            "        [ 0.4407,  0.3108,  0.3531,  0.2960,  0.5814],\n",
            "        [-0.0175,  1.0421,  0.3937,  0.6893,  0.5553],\n",
            "        [-0.2100, -0.0593,  0.0507, -0.0673, -0.2046]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJ_Z1l6yBzHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}