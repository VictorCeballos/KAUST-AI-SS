{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53pepfXZjw9B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import gdown\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets.utils import verify_str_arg\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class to download dataset\n",
    "class FossilNET(ImageFolder):\n",
    "    \"\"\"`FossilNET <https://github.com/softwareunderground/fossilnet>`_ Dataset.\n",
    "    Modified from https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``EMNIST/processed/training.pt``\n",
    "            and  ``EMNIST/processed/test.pt`` exist.\n",
    "        split (string): The dataset has 3 different splits: ``train``, ``val``,\n",
    "            ``test``. This argument specifies\n",
    "            which one to use.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    url = 'https://drive.google.com/uc?id=1_2TK0hC_b3mliXajPcobV-eq-3HNXd3q'\n",
    "    folder = 'fossilnet-png-224px'\n",
    "    md5 = '83e4f09fc78e3fd996c4e611c2653bf9'\n",
    "    splits = ('train', 'val', 'test')\n",
    "\n",
    "    def __init__(self, root, split, download=False, **kwargs):\n",
    "        self.split = verify_str_arg(split, \"split\", self.splits)\n",
    "        self.basedir = root\n",
    "        os.makedirs(self.basedir, exist_ok=True)\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.root = self.train_folder\n",
    "        elif self.split == \"val\":\n",
    "            self.root = self.val_folder\n",
    "        elif self.split == \"test\":\n",
    "            self.root = self.test_folder\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        super(FossilNET, self).__init__(self.root, **kwargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = super().__getitem__(index)\n",
    "        return item[0]\n",
    "        \n",
    "    @property\n",
    "    def train_folder(self):\n",
    "        return os.path.join(self.basedir, self.folder, 'train')\n",
    "\n",
    "    @property\n",
    "    def val_folder(self):\n",
    "        return os.path.join(self.basedir, self.folder, 'val')\n",
    "\n",
    "    @property\n",
    "    def test_folder(self):\n",
    "        return os.path.join(self.basedir, self.folder, 'test')\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(self.root)\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the FossilNET data if it doesn't exist already.\"\"\"\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        print('Downloading...')\n",
    "        # download files\n",
    "        gdown.download(self.url, os.path.join(self.basedir, self.__class__.__name__+\".zip\"), quiet=False)\n",
    "        # Unzipping the file\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(os.path.join(self.basedir, self.__class__.__name__+\".zip\"), 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.basedir)\n",
    "        print('Done!')\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"Split: {}\".format(self.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(75),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Get the training, validation and test data\n",
    "training_data = FossilNET('dataset', 'train', transform=transform, download=True)\n",
    "print(f'Training samples: {len(training_data)}')\n",
    "print(f'Training sample size: {training_data[0].shape}')\n",
    "\n",
    "# Randomly select a small subset of samples\n",
    "# training_data = torch.utils.data.Subset(training_data, np.random.choice(len(training_data), 600, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display results\n",
    "def display_image_grid(images, num_rows, num_cols, title_text=None):\n",
    "\n",
    "    fig = plt.figure(figsize=(num_cols*3., num_rows*3.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_cols), axes_pad=0.15)\n",
    "\n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.imshow(im.permute(1, 2, 0), interpolation='none')\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    if title_text is not None:\n",
    "        plt.suptitle(title_text, fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images\n",
    "images = []\n",
    "cols, rows = 5, 1\n",
    "for i in range(cols * rows):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img = training_data[sample_idx]\n",
    "    images.append(img)\n",
    "display_image_grid(images, rows, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm3hxkkqnu_R"
   },
   "source": [
    "# Image Generation using Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvASWvJK4ty-"
   },
   "source": [
    "The goal of Autoencoders was to generate latent vectors which are easily decodable back to the original image. But this can cause the latent space to become disjoint and non-continous.\n",
    "\n",
    "Variational Autoencoders were introduced to remedy this. Variational Autoencoders are trained to learn the probability distribution that models the input data. In varional autoencoders, the inputs are mapped to a probability distribuion over latent vectors. Typically, this probability distribution is set to standard normal distribution. So, there are two targets in VAEs: one to minimize the reconstruction loss and two to minimize the KL divergence between the probilitistic encoder and standard normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZW2SP07325K"
   },
   "source": [
    "![\"Variational Autoencoder Architecture\"](https://blog.bayeslabs.co/assets/img/vae-gaussian.png)\n",
    "\n",
    "<p align = \"center\">\n",
    "Fig.1 - Variational Autoencoder Architecture\n",
    "(<a href=https://blog.bayeslabs.co/2019/06/04/All-you-need-to-know-about-Vae\">\n",
    "source\n",
    "</a>)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1666089240908,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     },
     "user_tz": -300
    },
    "id": "Uezgo6PwuTMB",
    "outputId": "40d80d24-a4fb-4e8e-e47d-ec874e88d13e"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vejjknDqXBn"
   },
   "source": [
    "Here, we will define a model for a Variational Autoencoder. It is mostly similar to Autoencoders with a few differences. Instead of the encoder outputing a deterministic latent space representation like in autoencoders, the encoder will now output a mean and log variance vector. The latent space representation will then be sampled using them. We output log variance instead of just variance because the varaince always has to be postive.\n",
    "\n",
    "The docoder is same as in autoencoders.\n",
    "\n",
    "For sampling the latent space representation, we use a reparameterization trick so that it is possible to backpropagate. The basic idea is given below in the Fig 2.\n",
    "\n",
    "![\"Reparameteriztion Trick\"](https://blog.bayeslabs.co/assets/img/vae_part_1_1.png)\n",
    "\n",
    "<p align = \"center\">\n",
    "Fig.2 - Reparameteriztion Trick\n",
    "(<a href=https://blog.bayeslabs.co/2019/06/04/All-you-need-to-know-about-Vae\">\n",
    "source\n",
    "</a>)\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1666089240909,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     },
     "user_tz": -300
    },
    "id": "eNCJkFQsuVST",
    "outputId": "df0661a5-7b30-4f71-83be-83abde75aba5"
   },
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        \n",
    "        num_channels = input_dim[0]\n",
    "        height = input_dim[1]\n",
    "        width = input_dim[2]\n",
    "\n",
    "        lat_h = height // 8\n",
    "        lat_w = width // 8\n",
    "        \n",
    "        self.encoder_top = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            # TO-DO: add 2 more conv+norm+relu blokcs\n",
    "\n",
    "            nn.Flatten(1,-1),\n",
    "            nn.Linear(128 * lat_h * lat_w, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.mu_layer = None  # TO-DO\n",
    "        self.logvar_layer = None  # TO-DO\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(512, 128 * lat_h * lat_w),\n",
    "            nn.BatchNorm1d(128 * lat_h * lat_w),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Unflatten(1, (128, lat_h, lat_w)),\n",
    "\n",
    "            # TO-DO: add 2 conv+norm+relu blokcs\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=num_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encoder(self, x):\n",
    "        pass  # TO-DO: apply same encoder for both mu, logvar, but different FC layers (Hint: use encoder_top)\n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return None  # TO-DO: transform distribution of random samples\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass  # TO-DO: encoder+sampling+decoder (Hint: return output, mu, logvar)\n",
    "\n",
    "    def test(self, x):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1666089240909,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     },
     "user_tz": -300
    },
    "id": "eNCJkFQsuVST",
    "outputId": "df0661a5-7b30-4f71-83be-83abde75aba5"
   },
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "z = 256\n",
    "model = VariationalAutoEncoder(input_dim=training_data[0].shape, latent_dim=z)\n",
    "model.to(device)\n",
    "print(model)\n",
    "summary(model, training_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "img = training_data[0].unsqueeze(0).repeat((2, 1, 1, 1)).to(device)\n",
    "_ = model.test(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fJCUHHLt8H8"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "KLD_WEIGHT = 1e-4 # 0.0012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buCU8mBfwmF8"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = None  # TO-DO (Hint: use LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRwupNxk711T"
   },
   "source": [
    "The loss funtion of Variational Autoencoders is called Evidence Lower Bound (ELBO) as it bounds the likelihood of the data which we want to maximise and it consists of two terms.\n",
    "$$ L = ReconstructionLoss + KLDivergence $$\n",
    "Reconstruction Loss is simply a measure of the likelihood of the reconstructed data output at the decoder. Typically, it is the mean squared error between input and output.\n",
    "\n",
    "KL Divergence here acts as a regulaizer term becuase it is constraint on the form of tour approximate posterior. Typically, it is assumed as standard normal distribution.\n",
    "\n",
    "$$KL(N(\\mu, \\sigma), N(0, 1)) = -0.5 * (1 + \\log \\sigma^2 - \\sigma^2 - \\mu^2)$$\n",
    "\n",
    "Complete derivation of the loss for VAEs can be studied [here](https://deepai.org/publication/tutorial-deriving-the-standard-variational-autoencoder-vae-loss-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVEaW7_CtNu"
   },
   "source": [
    "Another thing we need to take care of is the weight of each term in the loss. If the weight of KL Divergence is too high, it will then take focus off reconstruction and we will get bad reconstruction and if it is too low, the approximate posterior distribution will become non-continous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLVJ3APiwsTs"
   },
   "outputs": [],
   "source": [
    "# Loss function for Variational Autoencoder\n",
    "def loss_fn(y_true, y_pred, mu, logvar):\n",
    "\n",
    "    # The total loss will consist of two parts: Reconstruction loss and KL divergence\n",
    "\n",
    "    # Reconstruction Loss is simply mean squared error of ground truth and prediction (as in autoencoders).\n",
    "    reconstruction_loss = nn.functional.mse_loss(y_pred, y_true)\n",
    "\n",
    "    # The second part of loss is the KL Divergence between the approximate posterior q(z|x) and latent prior p(z) assuming them to be normal distributions.\n",
    "    kld_loss = None  # TO-DO\n",
    "\n",
    "    loss = None  # TO-DO (Hint: use KLD_WEIGHT)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkSc77N5uPXA"
   },
   "outputs": [],
   "source": [
    "# Initialize PyTorch data loader\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 496056,
     "status": "ok",
     "timestamp": 1666089737851,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     },
     "user_tz": -300
    },
    "id": "55DPAA8P1qqE",
    "outputId": "aef631b0-880c-4090-b00c-316a8002833a"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "loss_history = []\n",
    "\n",
    "for i in range(1,EPOCHS+1):\n",
    "    \n",
    "    running_loss = 0\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    \n",
    "    for b, data in enumerate(pbar):\n",
    "        \n",
    "        # Get the data instances\n",
    "        inputs = data\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Zero the gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs, mu, logvar = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(inputs, outputs, mu, logvar)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update Progress\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_description(f\"Epoch {i}/{EPOCHS}: \")\n",
    "        pbar.set_postfix({\"loss\": loss.item(), \"avloss\": running_loss/(b+1)})\n",
    "    \n",
    "    loss_history.append(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the loss function\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.plot(loss_history, 'k', lw=3)\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666089737854,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     },
     "user_tz": -300
    },
    "id": "Rajsj2B52Jpk",
    "outputId": "dc6ecaf8-0032-4ee1-e6f0-9b5b64a75289"
   },
   "outputs": [],
   "source": [
    "# Set the model to eval state\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1666089740061,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     },
     "user_tz": -300
    },
    "id": "5f9raaWk2L81",
    "outputId": "9cdb5984-d8f0-4790-af8a-87ee0cf1b174"
   },
   "outputs": [],
   "source": [
    "# Display some sample images and there reconstruction through autoencoders\n",
    "images = []\n",
    "cols, rows = 7, 1\n",
    "for i in range(cols * rows):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img = training_data[sample_idx]\n",
    "    images.append(img)\n",
    "display_image_grid(images, rows, cols, \"original_images\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = model(torch.stack(images).to(device))[0].cpu()\n",
    "display_image_grid(reconstructed_images.squeeze(1), rows, cols, \"reconstructed_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRw2nEWX2WWq"
   },
   "source": [
    "## Generation through Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 3047,
     "status": "ok",
     "timestamp": 1666089984285,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     },
     "user_tz": -300
    },
    "id": "N7dzhdmi2Pzj",
    "outputId": "3d59c302-c8c3-4dcc-8142-900a9f3e4ffc"
   },
   "outputs": [],
   "source": [
    "# Now we will try some actual generation. We will sample the encodings randomly and then pass them through decoder\n",
    "rows, cols = 2, 7\n",
    "sample_encodings = torch.randn(rows*cols, z).to(device)\n",
    "with torch.no_grad():\n",
    "    generations = model.decoder(sample_encodings).cpu()\n",
    "display_image_grid(generations.squeeze(1), rows, cols, \"generated_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIw6-C9_GnMc"
   },
   "source": [
    "With Variational Autoencoders, we sacrificed a little in reconstruction but in the end we got some decent generations from completely random inputs. But as it can be seen, they are blurry. This is one of the shortcommings of VAEs, that they produce blurry images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOQbv+kcLQ+ycEnqvyJ8Exg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
